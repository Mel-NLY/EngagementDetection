{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Taken from CNN-LSTM/Video-Classification-CNN-and-LSTM--master/train_CNN_RNN.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dropout, Flatten, Dense, Conv2D,Conv3D, MaxPooling2D, TimeDistributed\n",
    "from keras import applications\n",
    "from keras.preprocessing import image \n",
    "from keras.optimizers import SGD\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.layers import LSTM\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "from scipy.misc import imread, imresize\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms,datasets, models\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "use_gpu = torch.cuda.is_available()\n",
    "if use_gpu:\n",
    "    pinMem = True\n",
    "else:\n",
    "    pinMem = False\n",
    "\n",
    "batch_size = 128\n",
    "\n",
    "path=r\"E:\\Documents\\Jobs\\NP Industry Project - Coddie\\Research on Behavioral Teaching\\Face Image Datasets\\DAiSEE\"\n",
    "traindf=pd.read_csv(path+\"/Labels/TrainFrameLabels.csv\",dtype=str)\n",
    "valdf=pd.read_csv(path+\"/Labels/ValidationFrameLabels.csv\",dtype=str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████| 1645782/1645782 [00:14<00:00, 111239.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25, 125, 224, 224, 3)\n",
      "(25, 125, 4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████| 516784/516784 [00:18<00:00, 27809.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7, 125, 224, 224, 3)\n",
      "(7, 125, 4)\n"
     ]
    }
   ],
   "source": [
    "# creating an empty list\n",
    "images = glob(path+\"\\DataSet\\Train\\TrainFrames/*.jpg\")\n",
    "trainpart_image = []\n",
    "train_image = []\n",
    "trainpart_label = []\n",
    "train_label = []\n",
    "\n",
    "# for loop to read and store frames\n",
    "for i in tqdm(range(traindf.shape[0])): \n",
    "    if i % 500 == 0:\n",
    "        # loading the image and keeping the target size as (224,224,3)\n",
    "        img = image.load_img(path+\"\\DataSet\\Train\\TrainFrames/\"+traindf['FrameID'][i], target_size=(224,224,3))\n",
    "        # converting it to array\n",
    "        img = image.img_to_array(img)\n",
    "        # normalizing the pixel value\n",
    "        img = img/255\n",
    "        # appending the image to the train_image list\n",
    "        trainpart_image.append(img)\n",
    "        trainpart_label.append(float(traindf['Engagement'][i]))\n",
    "    if i % 62500 == 0:\n",
    "        train_image.append(trainpart_image)\n",
    "        train_label.append(trainpart_label)\n",
    "        trainpart_image=[]\n",
    "        trainpart_label=[]\n",
    "        \n",
    "# converting the list to numpy array\n",
    "X_train = np.stack(train_image[1:-1])\n",
    "y_train = np.array(train_label[1:-1])\n",
    "y_train = to_categorical(y_train)\n",
    "\n",
    "# shape of the array\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "\n",
    "# creating an empty list\n",
    "images = glob(path+\"\\DataSet\\Validation\\ValidationFrames/*.jpg\")\n",
    "validationpart_image = []\n",
    "validation_image = []\n",
    "validationpart_label = []\n",
    "validation_label = []\n",
    "\n",
    "# for loop to read and store frames\n",
    "for i in tqdm(range(valdf.shape[0])):\n",
    "    if i % 500 == 0:\n",
    "        # loading the image and keeping the target size as (224,224,3)\n",
    "        img = image.load_img(path+\"\\DataSet\\Validation\\ValidationFrames/\"+valdf['FrameID'][i], target_size=(224,224,3))\n",
    "        # converting it to array\n",
    "        img = image.img_to_array(img)\n",
    "        # normalizing the pixel value\n",
    "        img = img/255\n",
    "        # appending the image to the train_image list\n",
    "        validationpart_image.append(img)\n",
    "        validationpart_label.append(float(valdf['Engagement'][i]))\n",
    "    if i % 62500 == 0:\n",
    "        validation_image.append(validationpart_image)\n",
    "        validation_label.append(validationpart_label)\n",
    "        validationpart_image=[]\n",
    "        validationpart_label=[]\n",
    "        \n",
    "# converting the list to numpy array\n",
    "X_val = np.stack(validation_image[1:-1])\n",
    "y_val = np.array(validation_label[1:-1])\n",
    "y_val = to_categorical(y_val)\n",
    "\n",
    "# shape of the array\n",
    "print(X_val.shape)\n",
    "print(y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "time_distributed_169 (TimeDi (None, 125, 224, 224, 32) 2432      \n",
      "_________________________________________________________________\n",
      "time_distributed_170 (TimeDi (None, 125, 220, 220, 32) 25632     \n",
      "_________________________________________________________________\n",
      "time_distributed_171 (TimeDi (None, 125, 110, 110, 32) 0         \n",
      "_________________________________________________________________\n",
      "time_distributed_172 (TimeDi (None, 125, 110, 110, 32) 0         \n",
      "_________________________________________________________________\n",
      "time_distributed_173 (TimeDi (None, 125, 110, 110, 8)  6408      \n",
      "_________________________________________________________________\n",
      "time_distributed_174 (TimeDi (None, 125, 106, 106, 8)  1608      \n",
      "_________________________________________________________________\n",
      "time_distributed_175 (TimeDi (None, 125, 53, 53, 8)    0         \n",
      "_________________________________________________________________\n",
      "time_distributed_176 (TimeDi (None, 125, 53, 53, 8)    0         \n",
      "_________________________________________________________________\n",
      "time_distributed_177 (TimeDi (None, 125, 22472)        0         \n",
      "_________________________________________________________________\n",
      "lstm_17 (LSTM)               (None, 125, 32)           2880640   \n",
      "_________________________________________________________________\n",
      "lstm_18 (LSTM)               (None, 125, 32)           8320      \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 125, 4)            132       \n",
      "=================================================================\n",
      "Total params: 2,925,172\n",
      "Trainable params: 2,925,172\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 25 samples, validate on 7 samples\n",
      "Epoch 1/30\n",
      "25/25 [==============================] - 2761s 110s/step - loss: 1.3875 - acc: 0.2416 - val_loss: 1.3623 - val_acc: 0.3566\n",
      "Epoch 2/30\n",
      "25/25 [==============================] - 2636s 105s/step - loss: 1.2647 - acc: 0.4806 - val_loss: 1.2973 - val_acc: 0.4057\n",
      "Epoch 3/30\n",
      "25/25 [==============================] - 2605s 104s/step - loss: 1.1726 - acc: 0.4835 - val_loss: 1.2359 - val_acc: 0.3531\n",
      "Epoch 4/30\n",
      "25/25 [==============================] - 2637s 105s/step - loss: 1.1118 - acc: 0.4781 - val_loss: 1.1935 - val_acc: 0.3634\n",
      "Epoch 5/30\n",
      "25/25 [==============================] - 3079s 123s/step - loss: 1.0751 - acc: 0.4835 - val_loss: 1.1661 - val_acc: 0.3531\n",
      "Epoch 6/30\n",
      "25/25 [==============================] - 3086s 123s/step - loss: 1.0506 - acc: 0.4966 - val_loss: 1.1486 - val_acc: 0.3531\n",
      "Epoch 7/30\n",
      "25/25 [==============================] - 2716s 109s/step - loss: 1.0334 - acc: 0.4998 - val_loss: 1.1368 - val_acc: 0.3966\n",
      "Epoch 8/30\n",
      "25/25 [==============================] - 2657s 106s/step - loss: 1.0213 - acc: 0.4896 - val_loss: 1.1277 - val_acc: 0.4754\n",
      "25/25 [==============================] - 3012s 120s/step - loss: 1.0096 - acc: 0.4966 - val_loss: 1.1204 - val_acc: 0.5086\n",
      "Epoch 10/30\n",
      "25/25 [==============================] - 2744s 110s/step - loss: 1.0012 - acc: 0.5005 - val_loss: 1.1144 - val_acc: 0.5280\n",
      "Epoch 11/30\n",
      "25/25 [==============================] - 2704s 108s/step - loss: 0.9940 - acc: 0.4966 - val_loss: 1.1091 - val_acc: 0.5234\n",
      "Epoch 12/30\n",
      "25/25 [==============================] - 2702s 108s/step - loss: 0.9880 - acc: 0.4989 - val_loss: 1.1049 - val_acc: 0.5269\n",
      "Epoch 13/30\n",
      "25/25 [==============================] - 2707s 108s/step - loss: 0.9824 - acc: 0.4890 - val_loss: 1.1017 - val_acc: 0.5257\n",
      "Epoch 14/30\n",
      "25/25 [==============================] - 2642s 106s/step - loss: 0.9774 - acc: 0.4899 - val_loss: 1.0988 - val_acc: 0.5246\n",
      "Epoch 15/30\n",
      "25/25 [==============================] - 2715s 109s/step - loss: 0.9721 - acc: 0.4979 - val_loss: 1.0961 - val_acc: 0.5246\n",
      "Epoch 16/30\n",
      "25/25 [==============================] - 2695s 108s/step - loss: 0.9681 - acc: 0.4832 - val_loss: 1.0931 - val_acc: 0.5303\n",
      "Epoch 17/30\n",
      "25/25 [==============================] - 2634s 105s/step - loss: 0.9636 - acc: 0.4970 - val_loss: 1.0912 - val_acc: 0.5303\n",
      "Epoch 18/30\n",
      "25/25 [==============================] - 2606s 104s/step - loss: 0.9597 - acc: 0.4947 - val_loss: 1.0892 - val_acc: 0.5303\n",
      "Epoch 19/30\n",
      "25/25 [==============================] - 2640s 106s/step - loss: 0.9567 - acc: 0.4902 - val_loss: 1.0873 - val_acc: 0.5280\n",
      "Epoch 20/30\n",
      "25/25 [==============================] - 2678s 107s/step - loss: 0.9524 - acc: 0.4973 - val_loss: 1.0849 - val_acc: 0.5269\n",
      "Epoch 21/30\n",
      "25/25 [==============================] - 2645s 106s/step - loss: 0.9501 - acc: 0.4957 - val_loss: 1.0835 - val_acc: 0.5246\n",
      "Epoch 22/30\n",
      "25/25 [==============================] - 2655s 106s/step - loss: 0.9465 - acc: 0.4950 - val_loss: 1.0821 - val_acc: 0.5246\n",
      "Epoch 23/30\n",
      "25/25 [==============================] - 2665s 107s/step - loss: 0.9436 - acc: 0.5027 - val_loss: 1.0809 - val_acc: 0.5257\n",
      "Epoch 24/30\n",
      "25/25 [==============================] - 2711s 108s/step - loss: 0.9413 - acc: 0.5082 - val_loss: 1.0802 - val_acc: 0.5280\n",
      "Epoch 25/30\n",
      "25/25 [==============================] - 2668s 107s/step - loss: 0.9391 - acc: 0.4954 - val_loss: 1.0787 - val_acc: 0.5257\n",
      "Epoch 26/30\n",
      "25/25 [==============================] - 2672s 107s/step - loss: 0.9370 - acc: 0.4928 - val_loss: 1.0771 - val_acc: 0.5291\n",
      "Epoch 27/30\n",
      "25/25 [==============================] - 2692s 108s/step - loss: 0.9352 - acc: 0.4928 - val_loss: 1.0770 - val_acc: 0.5257\n",
      "Epoch 28/30\n",
      "25/25 [==============================] - 2711s 108s/step - loss: 0.9327 - acc: 0.4922 - val_loss: 1.0760 - val_acc: 0.5269\n",
      "Epoch 29/30\n",
      "25/25 [==============================] - 2689s 108s/step - loss: 0.9307 - acc: 0.4854 - val_loss: 1.0755 - val_acc: 0.5257\n",
      "Epoch 30/30\n",
      "25/25 [==============================] - 2687s 107s/step - loss: 0.9293 - acc: 0.4883 - val_loss: 1.0746 - val_acc: 0.5269\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x20dc3fb9fd0>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "input_shape = (125, 224, 224, 3)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(TimeDistributed(Conv2D(32, (5, 5), padding='same', activation='relu'),input_shape=input_shape))\n",
    "model.add(TimeDistributed(Conv2D(32, (5, 5), activation='relu')))\n",
    "model.add(TimeDistributed(MaxPooling2D(pool_size=(2, 2))))\n",
    "model.add(TimeDistributed(Dropout(0.5)))\n",
    "model.add(TimeDistributed(Conv2D(8, (5, 5), padding='same', activation='relu')))\n",
    "model.add(TimeDistributed(Conv2D(8, (5, 5), activation='relu')))\n",
    "model.add(TimeDistributed(MaxPooling2D(pool_size=(2, 2))))\n",
    "model.add(TimeDistributed(Dropout(0.5)))\n",
    "model.add(TimeDistributed(Flatten()))\n",
    "\n",
    "model.add(LSTM(32, return_sequences=True))\n",
    "model.add(LSTM(32, return_sequences=True))\n",
    "model.add(Dense(4, activation='softmax'))\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "sgd = SGD(lr=0.00005, decay = 1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(optimizer=sgd, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "callbacks = [ EarlyStopping(monitor='val_loss', patience=10, verbose=0), ModelCheckpoint('video_1_LSTM_1_1024.h5', monitor='val_loss', save_best_only=True, verbose=0) ]\n",
    "nb_epoch = 30\n",
    "\n",
    "model.fit(X_train,y_train,validation_data=(X_val,y_val),batch_size=1, epochs=nb_epoch,callbacks=callbacks,verbose=1)\n",
    "#model.fit_generator(generator=train_generator(size=size, batch_size=batch_size), steps_per_epoch = 1645782 // 30,epochs = 10, validation_data=validation_generator, validation_steps = 1516784 // 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"model_1.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████| 536416/536416 [00:23<00:00, 22607.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7, 125, 224, 224, 3)\n",
      "(7, 125, 4)\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.models.load_model('model_1.h5')\n",
    "\n",
    "path=r\"E:\\Documents\\Jobs\\NP Industry Project - Coddie\\Research on Behavioral Teaching\\Face Image Datasets\\DAiSEE\"\n",
    "testdf=pd.read_csv(path+\"/Labels/TestFrameLabels.csv\",dtype=str)\n",
    "\n",
    "# creating an empty list\n",
    "images = glob(path+\"\\DataSet\\Test\\TestFrames/*.jpg\")\n",
    "testpart_image = []\n",
    "test_image = []\n",
    "testpart_label = []\n",
    "test_label = []\n",
    "\n",
    "# for loop to read and store frames\n",
    "for i in tqdm(range(testdf.shape[0])):\n",
    "    if i % 500 == 0:\n",
    "        # loading the image and keeping the target size as (224,224,3)\n",
    "        img = image.load_img(path+\"\\DataSet\\Test\\TestFrames/\"+testdf['FrameID'][i], target_size=(224,224,3))\n",
    "        # converting it to array\n",
    "        img = image.img_to_array(img)\n",
    "        # normalizing the pixel value\n",
    "        img = img/255\n",
    "        # appending the image to the train_image list\n",
    "        testpart_image.append(img)\n",
    "        testpart_label.append(float(testdf['Engagement'][i]))\n",
    "    if i % 62500 == 0:\n",
    "        test_image.append(testpart_image)\n",
    "        test_label.append(testpart_label)\n",
    "        testpart_image=[]\n",
    "        testpart_label=[]\n",
    "        \n",
    "# converting the list to numpy array\n",
    "X_test = np.stack(test_image[1:-1])\n",
    "y_test = np.array(test_label[1:-1])\n",
    "y_test = to_categorical(y_test)\n",
    "\n",
    "# shape of the array\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.5268571376800537\n",
      "Test Loss: 0.9409289530345372\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = model.evaluate(X_test,  y_test, batch_size=1, verbose=2)\n",
    "\n",
    "print(\"Test Accuracy: \" + str(test_acc))\n",
    "print(\"Test Loss: \" + str(test_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"model_1test.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'''\n",
    "def test_on_whole_videos(train_data,train_labels,validation_data,validation_labels):\n",
    "    parent = os.listdir(\"/Users/.../video/test\")\n",
    "    #.....................................Testing on whole videos.................................................................\n",
    "    x = []\n",
    "    y = []\n",
    "    count = 0\n",
    "    output = 0\n",
    "    count_video = 0\n",
    "    correct_video = 0\n",
    "    total_video = 0\n",
    "    base_model = load_VGG16_model()\n",
    "    model = train_model(train_data,train_labels,validation_data,validation_labels)\n",
    "    for video_class in parent[1:]:\n",
    "        print (video_class)\n",
    "        child = os.listdir(\"/Users/.../video/test\" + \"/\" + video_class)\n",
    "        for class_i in child[1:]:\n",
    "            sub_child = os.listdir(\"/Users/.../video/test\" + \"/\" + video_class + \"/\" + class_i)\n",
    "            for image_fol in sub_child[1:]:\n",
    "                if (video_class ==  'class_4' ):\n",
    "                    if(count%4 == 0):\n",
    "                        image = imread(\"/Users/.../video/test\" + \"/\" + video_class + \"/\" + class_i + \"/\" + image_fol)\n",
    "                        image = imresize(image , (224,224))\n",
    "\n",
    "                        x.append(image)\n",
    "                        y.append(output)\n",
    "                        #cv2.imwrite('/Users/.../video/validate/' + video_class + '/' + str(count) + '_' + image_fol,image)\n",
    "                    count+=1\n",
    "\n",
    "                else:\n",
    "                    if(count%4 == 0):\n",
    "                        image = imread(\"/Users/.../video/test\" + \"/\" + video_class + \"/\" + class_i + \"/\" + image_fol)\n",
    "                        image = imresize(image , (224,224))\n",
    "                        x.append(image)\n",
    "                        y.append(output)\n",
    "                        #cv2.imwrite('/Users/.../video/validate/' + video_class + '/' + str(count) + '_' + image_fol,image)\n",
    "                    count+=1\n",
    "            #correct_video+=1\n",
    "            x = np.array(x)\n",
    "            y = np.array(y)\n",
    "            x_features = base_model.predict(x)\n",
    "            #np.save(open('feat_' + 'class_' + str(output) + '_' + str(count_video) +'_'  + '.npy','w'),x)\n",
    "\n",
    "            correct = 0\n",
    "\n",
    "            answer = model.predict(x_features)\n",
    "            for i in range(len(answer)):\n",
    "                if(y[i] == np.argmax(answer[i])):\n",
    "                    correct+=1\n",
    "            print (correct,\"correct\",len(answer))\n",
    "            total_video+=1\n",
    "            if(correct>= len(answer)/2):\n",
    "                correct_video+=1\n",
    "            x = []\n",
    "            y = []\n",
    "            count_video+=1\n",
    "        output+=1\n",
    "\n",
    "    print (\"correct_video\",correct_video,\"total_video\",total_video)\n",
    "    print (\"The accuracy for video classification of \",total_video, \" videos is \", (correct_video/total_video))\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    train_generator,validation_generator = bring_data_from_directory()\n",
    "    base_model = load_VGG16_model()\n",
    "    train_data,train_labels,validation_data,validation_labels = extract_features_and_store(train_generator,validation_generator,base_model)\n",
    "    train_model(train_data,train_labels,validation_data,validation_labels)\n",
    "    test_on_whole_videos(train_data,train_labels,validation_data,validation_labels)\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
