{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Taken from CNN-LSTM/Video-Classification-CNN-and-LSTM--master/train_CNN_RNN.py\n",
    "#With smaller batches\n",
    "#With Pushbullet notifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mel/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/mel/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/mel/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/mel/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/mel/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/mel/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/mel/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/mel/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/mel/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/mel/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/mel/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/mel/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dropout, Flatten, Dense, Conv2D,Conv3D, MaxPooling2D, TimeDistributed\n",
    "from keras import applications\n",
    "from keras.preprocessing import image \n",
    "from keras.optimizers import SGD, Adam\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.layers import LSTM\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.utils import compute_class_weight\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "from scipy.misc import imread, imresize\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms,datasets, models\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "import time\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    " \n",
    "def send_notification_via_pushbullet(title, body):\n",
    "    \"\"\" Sending notification via pushbullet.\n",
    "        Args:\n",
    "            title (str) : title of text.\n",
    "            body (str) : Body of text.\n",
    "    \"\"\"\n",
    "    data_send = {\"type\": \"note\", \"title\": title, \"body\": body}\n",
    " \n",
    "    ACCESS_TOKEN = 'o.A174Gezu4JEGQgO1q8ChR7Xy40H2WbG9'\n",
    "    resp = requests.post('https://api.pushbullet.com/v2/pushes', data=json.dumps(data_send),\n",
    "                         headers={'Authorization': 'Bearer ' + ACCESS_TOKEN, 'Content-Type': 'application/json'})\n",
    "    if resp.status_code != 200:\n",
    "        raise Exception('Something wrong')\n",
    "    else:\n",
    "        print ('complete sending')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "use_gpu = torch.cuda.is_available()\n",
    "if use_gpu:\n",
    "    pinMem = True\n",
    "else:\n",
    "    pinMem = False\n",
    "\n",
    "batch_size = 128\n",
    "\n",
    "path=r\"/media/mel/MEL/Documents/Jobs/NP Industry Project - Coddie/Research on Behavioral Teaching/Face Image Datasets/DAiSEE\"\n",
    "traindf=pd.read_csv(path+\"/Labels/TrainFrameLabels.csv\",dtype=str)\n",
    "valdf=pd.read_csv(path+\"/Labels/ValidationFrameLabels.csv\",dtype=str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 65782/65782 [01:23<00:00, 783.91it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(218, 30, 224, 224, 3)\n",
      "(218, 30, 4)\n",
      "complete sending\n"
     ]
    }
   ],
   "source": [
    "# creating an empty list\n",
    "images = glob(path+\"/DataSet/Train/TrainFrames/*.jpg\")\n",
    "trainpart_image = []\n",
    "train_image = []\n",
    "trainpart_label = []\n",
    "train_label = []\n",
    "\n",
    "# for loop to read and store frames\n",
    "for i in tqdm(range(traindf.shape[0]-1580000)):\n",
    "    if i % 10 == 0:\n",
    "        # loading the image and keeping the target size as (224,224,3)\n",
    "        img = image.load_img(path+\"/DataSet/Train/TrainFrames/\"+traindf['FrameID'][i], target_size=(224,224,3))\n",
    "        # converting it to array\n",
    "        img = image.img_to_array(img)\n",
    "        # normalizing the pixel value\n",
    "        img = img/255\n",
    "        # appending the image to the train_image list\n",
    "        trainpart_image.append(img)\n",
    "        trainpart_label.append(float(traindf['Engagement'][i]))\n",
    "    if i % 300 == 0:\n",
    "        train_image.append(trainpart_image)\n",
    "        train_label.append(trainpart_label)\n",
    "        trainpart_image=[]\n",
    "        trainpart_label=[]\n",
    "        \n",
    "# converting the list to numpy array\n",
    "X_train = np.stack(train_image[1:-1])\n",
    "y_train = np.array(train_label[1:-1])\n",
    "y_train = to_categorical(y_train)\n",
    "\n",
    "# shape of the array\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "\n",
    "# Sending a notification when the sets are prepared\n",
    "send_notification_via_pushbullet(\"Time: \"+str(datetime.now())+\"\\nTraining & Validation Set\", \"X Training Images shape: \"+str(X_train.shape)+\"\\nY Training Labels shape: \"+str(y_train.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 36784/36784 [00:44<00:00, 828.34it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(121, 30, 224, 224, 3)\n",
      "(121, 30, 4)\n",
      "complete sending\n"
     ]
    }
   ],
   "source": [
    "# creating an empty list\n",
    "images = glob(path+\"/DataSet\\Validation/ValidationFrames/*.jpg\")\n",
    "validationpart_image = []\n",
    "validation_image = []\n",
    "validationpart_label = []\n",
    "validation_label = []\n",
    "\n",
    "# for loop to read and store frames\n",
    "for i in tqdm(range(valdf.shape[0]-480000)):\n",
    "    if i % 10 == 0:\n",
    "        # loading the image and keeping the target size as (224,224,3)\n",
    "        img = image.load_img(path+\"/DataSet/Validation/ValidationFrames/\"+valdf['FrameID'][i], target_size=(224,224,3))\n",
    "        # converting it to array\n",
    "        img = image.img_to_array(img)\n",
    "        # normalizing the pixel value\n",
    "        img = img/255\n",
    "        # appending the image to the train_image list\n",
    "        validationpart_image.append(img)\n",
    "        validationpart_label.append(float(valdf['Engagement'][i]))\n",
    "    if i % 300 == 0:\n",
    "        validation_image.append(validationpart_image)\n",
    "        validation_label.append(validationpart_label)\n",
    "        validationpart_image=[]\n",
    "        validationpart_label=[]\n",
    "        \n",
    "# converting the list to numpy array\n",
    "X_val = np.stack(validation_image[1:-1])\n",
    "y_val = np.array(validation_label[1:-1])\n",
    "y_val = to_categorical(y_val)\n",
    "\n",
    "# shape of the array\n",
    "print(X_val.shape)\n",
    "print(y_val.shape)\n",
    "\n",
    "# Sending a notification when the sets are prepared\n",
    "send_notification_via_pushbullet(\"Time: \"+str(datetime.now())+\"\\nTraining & Validation Set\", \"X Val Images shape: \"+str(X_val.shape)+\"\\nY Val Labels shape: \"+str(y_val.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/mel/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4070: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "time_distributed_1 (TimeDist (None, 30, 224, 224, 32)  2432      \n",
      "_________________________________________________________________\n",
      "time_distributed_2 (TimeDist (None, 30, 220, 220, 32)  25632     \n",
      "_________________________________________________________________\n",
      "time_distributed_3 (TimeDist (None, 30, 110, 110, 32)  0         \n",
      "_________________________________________________________________\n",
      "time_distributed_4 (TimeDist (None, 30, 110, 110, 32)  0         \n",
      "_________________________________________________________________\n",
      "time_distributed_5 (TimeDist (None, 30, 110, 110, 8)   6408      \n",
      "_________________________________________________________________\n",
      "time_distributed_6 (TimeDist (None, 30, 106, 106, 8)   1608      \n",
      "_________________________________________________________________\n",
      "time_distributed_7 (TimeDist (None, 30, 53, 53, 8)     0         \n",
      "_________________________________________________________________\n",
      "time_distributed_8 (TimeDist (None, 30, 53, 53, 8)     0         \n",
      "_________________________________________________________________\n",
      "time_distributed_9 (TimeDist (None, 30, 22472)         0         \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 30, 32)            2880640   \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 30, 32)            8320      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 30, 4)             132       \n",
      "=================================================================\n",
      "Total params: 2,925,172\n",
      "Trainable params: 2,925,172\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "WARNING:tensorflow:From /home/mel/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From /home/mel/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Train on 218 samples, validate on 121 samples\n",
      "Epoch 1/30\n"
     ]
    }
   ],
   "source": [
    "# Input (No. Video Samples, Timesteps, Dimension of Image, Channels)\n",
    "input_shape = (30, 224, 224, 3)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(TimeDistributed(Conv2D(32, (5, 5), padding='same', activation='relu'),input_shape=input_shape))\n",
    "model.add(TimeDistributed(Conv2D(32, (5, 5), activation='relu')))\n",
    "model.add(TimeDistributed(MaxPooling2D(pool_size=(2, 2))))\n",
    "model.add(TimeDistributed(Dropout(0.5)))\n",
    "model.add(TimeDistributed(Conv2D(8, (5, 5), padding='same', activation='relu')))\n",
    "model.add(TimeDistributed(Conv2D(8, (5, 5), activation='relu')))\n",
    "model.add(TimeDistributed(MaxPooling2D(pool_size=(2, 2))))\n",
    "model.add(TimeDistributed(Dropout(0.5)))\n",
    "model.add(TimeDistributed(Flatten()))\n",
    "\n",
    "model.add(LSTM(32, return_sequences=True))\n",
    "model.add(LSTM(32, return_sequences=True))\n",
    "model.add(Dense(4, activation='softmax'))\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "sgd = SGD(lr=0.00005, decay = 1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(optimizer=sgd, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "callbacks = [ EarlyStopping(monitor='val_loss', patience=10, verbose=0), ModelCheckpoint('video_1_LSTM_1_1024.h5', monitor='val_loss', save_best_only=True, verbose=0) ]\n",
    "nb_epoch = 30\n",
    "\n",
    "model.fit(X_train,y_train,validation_data=(X_val,y_val),batch_size=1 epochs=nb_epoch,callbacks=callbacks,verbose=1, shuffle=True)\n",
    "#model.fit_generator(generator=train_generator(size=size, batch_size=batch_size), steps_per_epoch = 1645782 // 30,epochs = 10, validation_data=validation_generator, validation_steps = 1516784 // 30)\n",
    "\n",
    "# Sending a notification when model finishes training\n",
    "send_notification_via_pushbullet(\"Time: \"+str(datetime.now())+\"\\nModel_2.h5 Finished Training\", str(model.summary()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"model_2.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model('model_2.h5')\n",
    "\n",
    "path=r\"/media/mel/MEL/Documents/Jobs/NP Industry Project - Coddie/Research on Behavioral Teaching/Face Image Datasets/DAiSEE\"\n",
    "testdf=pd.read_csv(path+\"/Labels/TestFrameLabels.csv\",dtype=str)\n",
    "\n",
    "# creating an empty list\n",
    "images = glob(path+\"/DataSet/Test/TestFrames/*.jpg\")\n",
    "testpart_image = []\n",
    "test_image = []\n",
    "testpart_label = []\n",
    "test_label = []\n",
    "\n",
    "# for loop to read and store frames\n",
    "for i in tqdm(range(testdf.shape[0])):\n",
    "    if i % 10 == 0:\n",
    "        # loading the image and keeping the target size as (224,224,3)\n",
    "        img = image.load_img(path+\"/DataSet/Test/TestFrames/\"+testdf['FrameID'][i], target_size=(224,224,3))\n",
    "        # converting it to array\n",
    "        img = image.img_to_array(img)\n",
    "        # normalizing the pixel value\n",
    "        img = img/255\n",
    "        # appending the image to the train_image list\n",
    "        testpart_image.append(img)\n",
    "        testpart_label.append(float(testdf['Engagement'][i]))\n",
    "    if i % 300 == 0:\n",
    "        test_image.append(testpart_image)\n",
    "        test_label.append(testpart_label)\n",
    "        testpart_image=[]\n",
    "        testpart_label=[]\n",
    "\n",
    "# converting the list to numpy array\n",
    "X_test = np.stack(test_image[1:-1])\n",
    "y_test = np.array(test_label[1:-1])\n",
    "y_test = to_categorical(y_test)\n",
    "\n",
    "# shape of the array\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc = model.evaluate(X_test,  y_test, batch_size=1, verbose=2)\n",
    "\n",
    "print(\"Test Accuracy: \" + str(test_acc))\n",
    "print(\"Test Loss: \" + str(test_loss))\n",
    "\n",
    "# Sending a notification when model finishes evaluation\n",
    "send_notification_via_pushbullet(\"Time: \"+str(datetime.now())+\"\\nModel_2.h5 Finished Evaluation\", \"Test Accuracy: \"+str(test_acc)+\"\\nTest Loss: \"+str(test_loss))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Perform each test\n",
    "for test_optimizer, label in tests:\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(loss=loss_function,\n",
    "                optimizer=test_optimizer,\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "    # Instantiate the Learning Rate Range Test / LR Finder\n",
    "    lr_finder = LRFinder(model)\n",
    "\n",
    "    # Perform the Learning Rate Range Test\n",
    "    outputs = lr_finder.find(input_train, target_train, start_lr=start_lr, end_lr=end_lr, batch_size=batch_size, epochs=no_epochs)\n",
    "\n",
    "    # Get values\n",
    "    learning_rates  = lr_finder.lrs\n",
    "    losses = lr_finder.losses\n",
    "    loss_changes = []\n",
    "\n",
    "    # Compute smoothed loss changes\n",
    "    # Inspired by Keras LR Finder: https://github.com/surmenok/keras_lr_finder/blob/master/keras_lr_finder/lr_finder.py\n",
    "    for i in range(moving_average, len(learning_rates)):\n",
    "    loss_changes.append((losses[i] - losses[i - moving_average]) / moving_average)\n",
    "\n",
    "    # Append values to container\n",
    "    test_learning_rates.append(learning_rates)\n",
    "    test_losses.append(losses)\n",
    "    test_loss_changes.append(loss_changes)\n",
    "    labels.append(label)\n",
    "\n",
    "    # for loop to read and store frames\n",
    "    for i in tqdm(range(testdf.shape[0])):\n",
    "    if i % 500 == 0:\n",
    "        # loading the image and keeping the target size as (224,224,3)\n",
    "        img = image.load_img(path+\"\\DataSet\\Test\\TestFrames/\"+testdf['FrameID'][i], target_size=(224,224,3))\n",
    "        # converting it to array\n",
    "        img = image.img_to_array(img)\n",
    "        # normalizing the pixel value\n",
    "        img = img/255\n",
    "        # appending the image to the train_image list\n",
    "        testpart_image.append(img)\n",
    "        testpart_label.append(float(testdf['Engagement'][i]))\n",
    "    if i % 62500 == 0:\n",
    "        test_image.append(testpart_image)\n",
    "        test_label.append(testpart_label)\n",
    "        testpart_image=[]\n",
    "        testpart_label=[]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'''\n",
    "def test_on_whole_videos(train_data,train_labels,validation_data,validation_labels):\n",
    "    parent = os.listdir(\"/Users/.../video/test\")\n",
    "    #.....................................Testing on whole videos.................................................................\n",
    "    x = []\n",
    "    y = []\n",
    "    count = 0\n",
    "    output = 0\n",
    "    count_video = 0\n",
    "    correct_video = 0\n",
    "    total_video = 0\n",
    "    base_model = load_VGG16_model()\n",
    "    model = train_model(train_data,train_labels,validation_data,validation_labels)\n",
    "    for video_class in parent[1:]:\n",
    "        print (video_class)\n",
    "        child = os.listdir(\"/Users/.../video/test\" + \"/\" + video_class)\n",
    "        for class_i in child[1:]:\n",
    "            sub_child = os.listdir(\"/Users/.../video/test\" + \"/\" + video_class + \"/\" + class_i)\n",
    "            for image_fol in sub_child[1:]:\n",
    "                if (video_class ==  'class_4' ):\n",
    "                    if(count%4 == 0):\n",
    "                        image = imread(\"/Users/.../video/test\" + \"/\" + video_class + \"/\" + class_i + \"/\" + image_fol)\n",
    "                        image = imresize(image , (224,224))\n",
    "\n",
    "                        x.append(image)\n",
    "                        y.append(output)\n",
    "                        #cv2.imwrite('/Users/.../video/validate/' + video_class + '/' + str(count) + '_' + image_fol,image)\n",
    "                    count+=1\n",
    "\n",
    "                else:\n",
    "                    if(count%4 == 0):\n",
    "                        image = imread(\"/Users/.../video/test\" + \"/\" + video_class + \"/\" + class_i + \"/\" + image_fol)\n",
    "                        image = imresize(image , (224,224))\n",
    "                        x.append(image)\n",
    "                        y.append(output)\n",
    "                        #cv2.imwrite('/Users/.../video/validate/' + video_class + '/' + str(count) + '_' + image_fol,image)\n",
    "                    count+=1\n",
    "            #correct_video+=1\n",
    "            x = np.array(x)\n",
    "            y = np.array(y)\n",
    "            x_features = base_model.predict(x)\n",
    "            #np.save(open('feat_' + 'class_' + str(output) + '_' + str(count_video) +'_'  + '.npy','w'),x)\n",
    "\n",
    "            correct = 0\n",
    "\n",
    "            answer = model.predict(x_features)\n",
    "            for i in range(len(answer)):\n",
    "                if(y[i] == np.argmax(answer[i])):\n",
    "                    correct+=1\n",
    "            print (correct,\"correct\",len(answer))\n",
    "            total_video+=1\n",
    "            if(correct>= len(answer)/2):\n",
    "                correct_video+=1\n",
    "            x = []\n",
    "            y = []\n",
    "            count_video+=1\n",
    "        output+=1\n",
    "\n",
    "    print (\"correct_video\",correct_video,\"total_video\",total_video)\n",
    "    print (\"The accuracy for video classification of \",total_video, \" videos is \", (correct_video/total_video))\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    train_generator,validation_generator = bring_data_from_directory()\n",
    "    base_model = load_VGG16_model()\n",
    "    train_data,train_labels,validation_data,validation_labels = extract_features_and_store(train_generator,validation_generator,base_model)\n",
    "    train_model(train_data,train_labels,validation_data,validation_labels)\n",
    "    test_on_whole_videos(train_data,train_labels,validation_data,validation_labels)\n",
    "\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
